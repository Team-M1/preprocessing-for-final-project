{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 코랩용 설정\r\n",
    "!git clone https://github.com/Team-M1/preprocessing-for-final-project.git\r\n",
    "%cd preprocessing-for-final-project\r\n",
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 코랩용 설정\r\n",
    "\r\n",
    "from google.colab import drive\r\n",
    "drive.mount('/content/drive')\r\n",
    "\r\n",
    "save_path = \"/content/drive/MyDrive/Colab Notebooks/model\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import torch\r\n",
    "from torch import nn\r\n",
    "\r\n",
    "\r\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# 데이터 불러오기\r\n",
    "\r\n",
    "df = pd.read_csv(\"./data/혐오표현_정제.csv\")\r\n",
    "df.head(2)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                content  hate  gender_hate\n",
       "0     걍 이 병신나라 민도가 좆박은건데 어쩔수가 잇나 걍 망해야지     1            0\n",
       "1  그 앰생 쓰레기 병신들은 짐승이고. 니는 짐승을 사람 취급해주냐.     1            0"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>hate</th>\n",
       "      <th>gender_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>걍 이 병신나라 민도가 좆박은건데 어쩔수가 잇나 걍 망해야지</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>그 앰생 쓰레기 병신들은 짐승이고. 니는 짐승을 사람 취급해주냐.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 만약 클래스별 가중치를 구하겠다면 사용\r\n",
    "\r\n",
    "# weight = df.shape[0] / (len(df[\"gender_hate\"].unique()) * np.bincount(df[\"gender_hate\"]))\r\n",
    "# weight = torch.from_numpy(weight).float()\r\n",
    "# weight"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.5651, 4.3406])"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# 모델 불러오기\r\n",
    "\r\n",
    "from transformers import ElectraForSequenceClassification, ElectraTokenizer\r\n",
    "from tokenization_kocharelectra import KoCharElectraTokenizer\r\n",
    "\r\n",
    "\r\n",
    "electramodel = ElectraForSequenceClassification.from_pretrained(\"monologg/kocharelectra-small-discriminator\")\r\n",
    "tokenizer = KoCharElectraTokenizer.from_pretrained(\"monologg/kocharelectra-small-discriminator\")\r\n",
    "electramodel = electramodel.to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at monologg/kocharelectra-small-discriminator were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/kocharelectra-small-discriminator and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'KoCharElectraTokenizer'.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# 데이터프레임을 TensorDataset으로 만들기\r\n",
    "\r\n",
    "from torch.utils.data import TensorDataset\r\n",
    "\r\n",
    "from data_preprocess import df_to_feature_and_label\r\n",
    "\r\n",
    "\r\n",
    "all_data = TensorDataset(*df_to_feature_and_label(df, tokenizer, max_length=256))\r\n",
    "\r\n",
    "lr = 0.001\r\n",
    "\r\n",
    "# criterion = torch.nn.CrossEntropyLoss(weight=weight.to(device))  # 클래스별 가중치가 적용된 크로스엔트로피로스\r\n",
    "criterion = torch.nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.AdamW(electramodel.classifier.parameters(), lr=lr)\r\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 코랩용 체크포인트 불러오기 설정\r\n",
    "saved_checkpoint = os.path.join(save_path, \"checkpoint210814.pth\")\r\n",
    "checkpoint = torch.load(saved_checkpoint)\r\n",
    "electramodel.load_state_dict(checkpoint[\"model_state_dict\"])\r\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\r\n",
    "scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# ImbalancedDatasetSampler에서 사용되는 함수\r\n",
    "# dataset[:][3]은 label값을 가리킨다.\r\n",
    "\r\n",
    "def get_labels(dataset):\r\n",
    "    return dataset[:][3]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from trainer import training\r\n",
    "\r\n",
    "\r\n",
    "training(\r\n",
    "    all_data,\r\n",
    "    electramodel,\r\n",
    "    criterion,\r\n",
    "    optimizer,\r\n",
    "    scheduler,\r\n",
    "    get_label=get_labels,\r\n",
    "    epochs=100,\r\n",
    "    batch_size=16,\r\n",
    "    save_path=save_path\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "c86e0eb5395ede85b9f59b6e8263bc6c22037c4e880f7255165769e612363282"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}